---
layout: post
title: "Claude vs Codex: Practical Guidance From Daily Use"
baseline: Strengths, failure modes, and how to use each effectively
credit: 'Generated by Nano Banana Pro with prompt “Wide, realistic photograph from the top of a ski mountain showing two diverging runs: one steep, straight black-diamond slope dropping sharply downward, and one gentler blue run curving off to the right. No people visible. Natural winter light, editorial photography style, restrained color grading”'
splash: media/wide-realistic-photograph-from-the-top-of-a-ski-mountain-showing-two-diverging-runs-one-steep-straight-black-diamond-slope-dropping-sharply-downward-and-one-gentler-blue-run-curving-off-to-the-right.avif
category: blog
published: true
tags: [coding, coding-agents, javascript, typescript]
---

I have been using AI coding agents heavily to amplify the output of our two-person product team at [Outlyne](https://outlyne.com). My co-founder, [Jeremy Willer,](https://willerdesign.com) is a brilliant product designer and highly capable traditional web developer (HTML and CSS), while I’m a full-stack software engineer. Over the last decade, React and JSX (now TSX) have enabled Jeremy to become a productive contributor to our Typescript codebase. Then when AI coding agents became [available and usable in Zed,](https://zed.dev/blog/fastest-ai-code-editor) our preferred editor, it was like we suddenly had an extremely productive junior engineer available around the clock to take on any coding task that previously would’ve fallen to me.

You’re almost certainly familiar with this story. A week later, I was staring at a backlog of PRs to review totalling more than 20K changed lines of code, and the overall code quality of the PRs was quite low. Claude Sonnet 3.7 was absolutely unrivaled at generating “working” code, and absolutely awful at structuring that code to be maintainable and resilient. My productivity ground to a halt as I spent the next two weeks just reviewing and fixing PRs, miserable and heading fast towards burnout. [Bugs made it into production]({% link _posts/2025-12-09-how-ai-coding-agents-hid-a-timebomb-in-our-app.md %}) and ultimately cost me weeks of debugging.

We quickly realized that we could use the agents to put together feature prototypes and try them out and gather feedback, but that we absolutely couldn’t use them to blindly generate PRs intended for deployment. “Blindly” is doing real work there: my fundamental rule is that all code must be seen (reviewed) by a human who understands what it’s doing. And we also realized that working through a plan or spec before starting a task was essential.

## Claude (Sonnet|Opus) 4.5

The agents have come a long way since then. Claude Sonnet 4.5 (and Opus 4.5 when needs demand it) is still unrivaled at code generation, and its instincts around maintainability have improved. We’ve also substantially tightened our `AGENTS.md` rules (symlinked as `CLAUDE.md`) to steer it towards our best practices and away from common mistakes, making the output easier to review and merge. We use it heavily to plan out features and rearchitectures and migrations, storing them as markdown files in a root `plans/` folder in our repo.

We also use it for implementing distinct tasks in those plans, or for simple one-off coding tasks we can describe with enough detail directly in the prompt, and while I rarely if ever have been able to one-shot anything of any significant complexity with a result to my satisfaction, claude can usually get 90% of the way there.

Where it consistently fails, however, is code review. It rarely finds any important issues and more often than not ends with baseless claims on how “production-ready” the code is, while the issues it _does_ raise are generally some particularly toxic combination of marginal in scope and misguided.

## OpenAI Codex

A couple of months ago, I was persuaded by Peter Steinberger’s [“Just Talk To It” blog post](https://steipete.me/posts/just-talk-to-it) to try out the `codex` CLI. Coming from `claude`, I was genuinely perplexed by Peter’s enthusiasm. I frequently found myself asking codex to do something clear and straightforward, checking in on something else in the meantime, then coming back to find that it had thought through a plan, listed a few bullet points, said it was _going to start…_ and then stopped. I kept having to say “go ahead” or “continue” and it was proving quite frustrating. I tried queueing up “commit and continue” commands like Peter recommends in that post, but I was never able to get `codex` able to git commit (some kind of permission issue), despite spending too long asking codex and then ChatGPT to help me resolve the issues. I still regularly see `/Users/andrew/.zlogin:9: nice(5) failed: operation not permitted` in codex’s output.

All that said, I have very much come to appreciate `codex` (mostly using `gpt-5.1-codex-max`, not `gpt-5.2-codex`). It has never told me that a PR is “production-ready”, nor does it constantly blow smoke up my ass. It’s much more reserved with praise and enthusiasm. It just feels like a more serious coding partner, especially in contrast to claude’s [genius-level golden retriever on acid.](https://www.youtube.com/shorts/ZxXGi2_5Pz0) I don’t use it as often for pure coding tasks, because I still find there’s more friction in the process. However, it occurred to me that the more serious nature of codex could lend itself to more effective code review. I then discovered that `codex` has a `/review` slash command that lets you review the current branch “PR-style” (against the base branch of your choosing), uncommitted changes, or a specific commit.

So I tried it out and holy crap. It’s sooooo much better than claude or gemini. Watching it go through the code changes is a trip. It will say _I'm reviewing `SectionContentEditor` and related hooks for handling undefined data safely, and considering stale data risks from `useEffect` dependencies and `useEffectEvent` usage. I'm also checking event schema changes, especially optional fields affecting migrations, and assessing whether materializers and tests properly incorporate new event fields like `websiteId`.`_ Leading me to say _thank god someone is paying attention._

![Example of a codex review]({{ site.base_url }}/media/codex-review-screenshot.avif)

## Gemini, Antigravity, and the Browser-as-a-Tool Future

I haven’t meaningfully used Gemini in production yet, so this isn’t an evaluation, but I want to call out one capability I’m increasingly convinced will matter a lot: first-class browser access. When Antigravity was announced and included Chrome as an agentic tool, I was stoked. So I tried it out and… it works ok. But I’ve tried using it to debug multiple in-browser issues and all three times, it failed to identify or fix the issue, and the editor was so buggy that I haven’t returned to it. For web dev, I think having a built-in browser that the agent can use as a tool is a huge deal, so I’m bullish long-term on Antigravity (and other tools like [Tidewave](https://tidewave.ai)).

My co-founder has tried gemini from within Zed and tentatively prefers it over claude, crediting it with responding to prompts with working code that solves the problem with less overall code generated. However, I’ve become refined enough with my prompts and comfortable enough with my usage that I know how to keep claude from going off the rails, so I don’t currently have a need there.

That’s how I’m using AI coding agents today. It will almost certainly change in a few months. I’m already suspicious—though not yet confident—that `gpt-5.2-codex` is a less thorough reviewer than `gpt-5.1-codex-max` based on how quickly it completes reviews and the scenarios it seems to consider.

What I’m more confident about is this: different agents have very different failure modes, and using them effectively means choosing where speed is acceptable and where scrutiny is non-negotiable. That distinction has mattered far more than any single model upgrade so far.
